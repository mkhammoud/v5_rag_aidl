{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "{Lesson 2: Introduction to Pandas Dataframe [300 mins]}\n\\subsection{Introduction}\nWelcome to the second lesson of the week. In our previous lesson, we delved into the world of NumPy, a powerful library for array manipulation. Understanding NumPy is vital, particularly when working with machine learning libraries that exclusively accept NumPy arrays as input.\n\nIn this lesson, we will further expand our knowledge with a focus on data analysis and exploration. Enter Pandas, a widely-used Python library that leverages DataFrames and Series for efficient data management. Pandas builds upon NumPy's capabilities and offers robust data handling and computation for various data types. With Pandas, we can effortlessly handle and analyze large datasets, making it an indispensable tool.\n\nSimilar to our exploration of NumPy, we will go together through an introductory journey into Pandas DataFrames and Series, gaining insights into their attributes and functionalities. Topics include loading data into Pandas DataFrames, data comprehension, statistical data extraction, and techniques for data selection, slicing, and grouping.\n\nRemember, becoming proficient in AI programming requires dedicated practice. As you follow along with this lesson, we highly encourage you to implement each concept in your Jupyter Notebook. Let's get started with Pandas!\n\n\n\n\\subsection{Understanding the Pandas Library and Its Role in Data Manipulation}\nPandas is a crucial Python library for data analysis and manipulation. With over 100 million monthly downloads, it is the go-to package for data scientists, offering a wide range of capabilities to work with tabular data, also known as DataFrames. In this lesson, we'll explore what Pandas is, its connection to NumPy, and its role in data manipulation for beginners in programming and applied AI.\n\n\\subsubsection{What is Pandas?}\nPandas is a Python library designed for the manipulation and analysis of tabular data. A DataFrame, which we can think of as an Excel sheet, is at the core of Pandas. While NumPy is excellent for numerical computing, Pandas complements it by providing efficient storage and manipulation of data with labels, heterogeneity, and missing values.\n\nPandas' functionality includes a wide range of data transformations, such as sorting rows, selecting subsets, and reshaping DataFrames. It is especially powerful when it comes to joining and merging DataFrames. In many ways, Pandas builds on top of NumPy, providing more flexible data structures for real-world data.\n\n\\subsubsection{Pandas and the PyData Ecosystem:}\nPandas seamlessly integrates with other Python data science packages, collectively known as the PyData ecosystem. This ecosystem includes:\n\n\\begin{enumerate}\n    \\item \\textbf{NumPy:} Pandas leverages NumPy for efficient numerical computing.\n    \\item \\textbf{Matplotlib, Seaborn, Plotly, and other data visualization packages:} Pandas enables you to visualize our data with ease. We will cover data visualization in Week \\#7.\n    \\item \\textbf{scikit-learn:} is a machine learning library. For machine learning tasks, Pandas can help preprocess and prepare our data. We will be covering \\textbf{scikit-learn:} library in Week \\#8.\n\\end{enumerate}\n\n\\subsubsection{What is Pandas Used For?}\nPandas plays a crucial role throughout the data analysis workflow. Here are some of its key uses that we are going to cover in this lesson:\n\n\\begin{enumerate}\n    \\item \\textbf{Importing Data:} We can import data from various sources, including databases, spreadsheets, CSV files, and more.\n    \\item \\textbf{Data Cleaning:} Pandas helps handle missing values and other data cleaning tasks.\n    \\item \\textbf{Data Transformation:} We can reshape and tidy datasets to make them suitable for analysis.\n    \\item \\textbf{Data Aggregation:} Calculate summary statistics, correlations, and more for our data.\n    \\item \\textbf{Data Visualization:} Pandas can be used to visualize our datasets, which can help us uncover insights using visualization libraries.\n\\end{enumerate}\n\n\\subsubsection{Key Benefits of Pandas:}\nPandas comes with several key benefits:\n\n\\begin{enumerate}\n    \\item \\textbf{Made for Python:} It's tailored to the Python language, which is widely used in data science and machine learning.\n    \\item \\textbf{Conciseness:} Pandas allows us to achieve complex data manipulations with concise and readable code.\n    \\item \\textbf{Intuitive Data Representation:} DataFrames provide an intuitive way to represent and work with tabular data, making it easier to understand and analyze.\n    \\item \\textbf{Extensive Features:} It offers a vast array of data operations, from exploratory data analysis to dealing with missing values, calculating statistics, and visualizing data.\n    \\item \\textbf{Scalability}: Pandas is efficient and can handle large datasets, even those with millions of records and hundreds of columns.\n\\end{enumerate}\nIn summary, Pandas is a powerful library that complements NumPy by providing strong tools for data manipulation and analysis. It's an essential package in the Python data science ecosystem, making it easier for beginners to get started with data analysis and AI-related tasks.\n\n\\subsection{Installing and Importing Pandas}\nTo install the Pandas library in Python, we can use the Python package manager, pip, which is a simple and common way to install Python packages. Type the following command in the terminal and press Enter:\n\\begin{lstlisting}[language=Python]\npip install pandas\n\\end{lstlisting}\nNote that Pandas should be already installed following the exercises of Week \\#4. \n\nWhen importing pandas, the most common alias for it is pd. Thus, we can use pd instead of writing Pandas everytime in our code. In a notebook cell, we can import pandas as follows:\n\\begin{lstlisting}[language=Python]\nimport pandas as pd\n\\end{lstlisting}\n\n\\subsection{Loading Data Into a Dataframe From CSV Files}\nImporting data into Pandas from different sources is a fundamental skill when working with data analysis and manipulation. Pandas provides various functions for importing data, including CSV files, text files, and Excel files. Here's a description of how to import data from a CSV file using Pandas:\n\n\\subsubsection{Importing CSV Files:}\nCSV (Comma-Separated Values) files are one of the most common data formats. We can use the \\textit{read\\_csv()} function to import data from a CSV file. Provide the path to the CSV file as an argument, and Pandas will create a DataFrame from the data. Here's an example:\n\\begin{lstlisting}[language=Python]\ndf = pd.read_csv(\"data.csv\")\n\\end{lstlisting}\nIn this example, the CSV file \"data.csv\" is read into a Pandas DataFrame named \"\\textit{df}\" which we can use for data analysis.\n\n\\subsection{Viewing and Understanding Data in Dataframes}\nAfter we've loaded our data into a DataFrame, it's important to take a look at what's inside. We can do this by either viewing a small part of the data or getting an overview of the numbers in our data. Pandas provides a wide range of functions and attributes to help us explore and understand our data effectively. In this introduction, we'll discuss some basic Pandas functions and attributes to view and comprehend DataFrames, which are the primary data structure in Pandas.\n\nTo see the use of Pandas in action, let's consider studying a new dataset which contains information to track the fitness of users through an app. Let's start working together to learn about Pandas. Download the \\textit{CardioGoodFitness.csv} dataset file and open a Kupyter notebook to get started. \n\n\\subsubsection{Load The Dataset}\n\\begin{lstlisting}[language=Python]\nimport pandas as pd\n\n# Load your dataset (in this example, 'CardioGoodFitness.csv')\ndf = pd.read_csv('./CardioGoodFitness.csv')\n\\end{lstlisting}\n\n\\subsubsection{Head and Tail:}\nUse \\textit{.head()} to view the first few rows of the DataFrame. By default, it shows the first 5 rows, providing a quick look at the beginning of our data.\n\n\\begin{lstlisting}[language=Python]\ndf.head()\n\\end{lstlisting}\n\nOutput:\n\\begin{figure}[hp!]\n    \\centering\n    \\includegraphics[width=\\textwidth]{Images/df.head.png}\n\\end{figure}\n\n%   Product\tAge\tGender\tEducation\tMaritalStatus\tUsage\tFitness\tIncome\tMiles\n% 0\tTM195\t18\tMale\t14\tSingle\t3\t4\t29562\t112\n% 1\tTM195\t19\tMale\t15\tSingle\t2\t3\t31836\t75\n% 2\tTM195\t19\tFemale\t14\tPartnered\t4\t3\t30699\t66\n% 3\tTM195\t19\tMale\t12\tSingle\t3\t3\t32973\t85\n% 4\tTM195\t20\tMale\t13\tPartnered\t4\t2\t35247\t47\n\nTo see the last 'n' rows of the DataFrame, we can use \\textit{.tail(n)} with 'n' indicating the number of rows we want to view. This helps us examine the most recent data.\n\n\\begin{lstlisting}[language=Python]\ndf.tail(n=10)\n\\end{lstlisting}\n\nOutput:\n\\begin{figure}[hp!]\n    \\centering\n    \\includegraphics[width=\\textwidth]{Images/df.tail.png}\n\\end{figure}\n\n%       Product\tAge\tGender\tEducation\tMaritalStatus\tUsage\tFitness\tIncome\tMiles\n% 170\tTM798\t31\tMale\t16\tPartnered\t6\t5\t89641\t260\n% 171\tTM798\t33\tFemale\t18\tPartnered\t4\t5\t95866\t200\n% 172\tTM798\t34\tMale\t16\tSingle\t5\t5\t92131\t150\n% 173\tTM798\t35\tMale\t16\tPartnered\t4\t5\t92131\t360\n% 174\tTM798\t38\tMale\t18\tPartnered\t5\t5\t104581\t150\n% 175\tTM798\t40\tMale\t21\tSingle\t6\t5\t83416\t200\n% 176\tTM798\t42\tMale\t18\tSingle\t5\t4\t89641\t200\n% 177\tTM798\t45\tMale\t16\tSingle\t5\t5\t90886\t160\n% 178\tTM798\t47\tMale\t18\tPartnered\t4\t5\t104581\t120\n% 179\tTM798\t48\tMale\t18\tPartnered\t4\t5\t95508\t180\n\n\\subsubsection{Summary Statistics:}\n\\subsubsection{describe()}\nThe \\textit{.describe()} method generates summary statistics for the numeric columns within our DataFrame. These statistics include count, mean, standard deviation, minimum, maximum, and quartiles, giving us an understanding of the central tendencies and spread of the data.\n\\begin{lstlisting}[language=Python]\ndf.describe()\n\\end{lstlisting}\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.describe.png}\n\\end{figure}\n%       Age\t        Education\tUsage\t    Fitness\t    Income\t     Miles\n% count\t180.000000\t180.000000\t180.000000\t180.000000\t180.000000\t180.000000\n% mean\t28.788889\t15.572222\t3.455556\t3.311111\t53719.577778\t103.194444\n% std\t6.943498\t1.617055\t1.084797\t0.958869\t16506.684226\t51.863605\n% min\t18.000000\t12.000000\t2.000000\t1.000000\t29562.000000\t21.000000\n% 25%\t24.000000\t14.000000\t3.000000\t3.000000\t44058.750000\t66.000000\n% 50%\t26.000000\t16.000000\t3.000000\t3.000000\t50596.500000\t94.000000\n% 75%\t33.000000\t16.000000\t4.000000\t4.000000\t58668.000000\t114.750000\n% max\t50.000000\t21.000000\t7.000000\t5.000000\t104581.000000\t360.000000\n\nTo customize the percentiles displayed in the summary, we can provide the percentiles argument, allowing us to focus on specific percentiles of interest. The percentiles (25\\%, 50\\%, and 75\\%) indicate values that divide the data into different segments based on their relative positions within the data distribution. These percentiles help us understand how data is spread and whether it has any skewness or outliers.\n\\begin{lstlisting}[language=Python]\ndf.describe(percentiles=[0.3, 0.5, 0.7])\n\\end{lstlisting}\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.describe.percentile.png}\n\\end{figure}\n\n%       Age\t        Education\tUsage\t    Fitness\t    Income\t     Miles\n% count\t180.000000\t180.000000\t180.000000\t180.000000\t180.000000\t180.000000\n% mean\t28.788889\t15.572222\t3.455556\t3.311111\t53719.577778\t103.194444\n% std\t6.943498\t1.617055\t1.084797\t0.958869\t16506.684226\t51.863605\n% min\t18.000000\t12.000000\t2.000000\t1.000000\t29562.000000\t21.000000\n% 30%\t24.700000\t14.000000\t3.000000\t3.000000\t45480.000000\t75.000000\n% 50%\t26.000000\t16.000000\t3.000000\t3.000000\t50596.500000\t94.000000\n% 70%\t31.300000\t16.000000\t4.000000\t4.000000\t55060.600000\t107.800000\n% max\t50.000000\t21.000000\t7.000000\t5.000000\t104581.000000\t360.000000\n\nAdditionally, we can use \\textit{include} and \\textit{exclude} arguments to specify which data types to consider or exclude in the summary. This allows us to tailor our analysis to the specific aspects of our data you want to explore. In the example below, we use \\textit{include=[int]} to only consider columns having an integer data type.\n\\begin{lstlisting}[language=Python]\ndf.describe(include=[int])\n\\end{lstlisting}\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/pd.include.int.png}\n\\end{figure}\n\n%       Age\t        Education\tUsage\t    Fitness\t    Income\t     Miles\n% count\t180.000000\t180.000000\t180.000000\t180.000000\t180.000000\t180.000000\n% mean\t28.788889\t15.572222\t3.455556\t3.311111\t53719.577778\t103.194444\n% std\t6.943498\t1.617055\t1.084797\t0.958869\t16506.684226\t51.863605\n% min\t18.000000\t12.000000\t2.000000\t1.000000\t29562.000000\t21.000000\n% 25%\t24.000000\t14.000000\t3.000000\t3.000000\t44058.750000\t66.000000\n% 50%\t26.000000\t16.000000\t3.000000\t3.000000\t50596.500000\t94.000000\n% 75%\t33.000000\t16.000000\t4.000000\t4.000000\t58668.000000\t114.750000\n% max\t50.000000\t21.000000\t7.000000\t5.000000\t104581.000000\t360.000000\n\nIn contrast, we write \\textit{exclude=[int]} as an argument to consider the data of all columns except the ones having an int datatype.\n\\begin{lstlisting}[language=Python]\ndf.describe(exclude=[int])\n\\end{lstlisting}\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.4\\textwidth]{Images/pd.exclude.int.png}\n\\end{figure}\n\n%       count\tmean\t     std\t    min\t    25%\t    50%\t    75%\t    max\n% Age\t180.0\t28.788889\t6.943498\t18.0\t24.00\t26.0\t33.00\t50.0\n% Education\t180.0\t15.572222\t1.617055\t12.0\t14.00\t16.0\t16.00\t21.0\n% Usage\t180.0\t3.455556\t1.084797\t2.0\t3.00\t3.0\t4.00\t7.0\n% Fitness\t180.0\t3.311111\t0.958869\t1.0\t3.00\t3.0\t4.00\t5.0\n% Income\t180.0\t53719.577778\t16506.684226\t29562.0\t44058.75\t50596.5\t58668.00\t104581.0\n% Miles\t180.0\t103.194444\t51.863605\t21.0\t66.00\t94.0\t114.75\t360.0\n\nFinally, we can transpose the summary statistics for better readability by appending \\textit{.T} to the \\textit{.describe()} method, making it easier to examine data at a glance. Simply, transpose rotates the data table by replacing rows by columns and vice versa. Let's consider the transpose of the default describe function call.\n\n\\begin{lstlisting}[language=Python]\ndf.describe().T\n\\end{lstlisting}\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/pd.transpose.png}\n\\end{figure}\n\n\\subsubsection{mean()}\nIn Pandas, we can compute the mean of a dataframe column using the df['column\\_name'].mean() expression. The result gives us the average numerical value of the rows in our dataset. Here's how it works:\n\\begin{lstlisting}[language=Python]\ndf['Age'].mean()\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\n28.816666666666666\n\\end{lstlisting}\nTherefore, we  now from this example that the average age of all users in the dataset is around 28.8.\n\nIn pandas, there exist a list of aggregation methods to extract statistical information about the data, in addition to the mean, including:\n\n\\begin{table}[htp!]\n    \\centering\n    \\begin{tabular}{l|l}\n        Aggregation & Description\\\\\n        \\hline\n        \\textit{count()} & Total number of items\\\\\n        \\textit{first(), last()} & First and last item\\\\\n        \\textit{min(), max()} & Minimum and maximum\\\\\n        \\textit{std(), var()} & Standard deviation and variance\\\\\n        \\textit{prod()} & Product of all items\\\\\n        \\textit{sum()} & Sum of all items\\\\\n    \\end{tabular}\n\\end{table}\n\n\\subsubsection{value\\_counts()}\nIn Pandas, we can use \\textit{df['column\\_name'].value\\_counts()} to count the number of occurrences of each category inside that column in a given dataset. This is a way to understand the distribution of the column categories in our data. Here's how it works:\n\n\\begin{lstlisting}[language=Python]\ndf['Fitness'].value_counts()\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\n3    97\n5    31\n2    26\n4    24\n1     2\nName: Fitness, dtype: int64\n\\end{lstlisting}\nIn this example, we can see the distribution of the Fitness score in the dataset, knowing that the largest Fitness value that occurs in the dataset is 3.\n\n\\subsubsection{idmax()}\nIn Pandas, \\textit{df['column\\_name'].idxmax()} is a function used to locate the position (index) of the maximum value in a specific column. This function helps us pinpoint the row or entry where the highest value occurs within that column. Let's try to find the index of the row with the highest age value:\n\n\\begin{lstlisting}[language=Python]\nindex = df['Age'].idxmax()\nprint(index)\nprint(df.iloc[index])\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\n79\nProduct              TM195\nAge                     50\nGender              Female\nEducation               16\nMaritalStatus    Partnered\nUsage                    3\nFitness                  3\nIncome               64809\nMiles                   66\nName: 79, dtype: object\n\\end{lstlisting}\nTherefore, the row with index 79 has the maximum age value in the dataset. We are using the iloc function to identify the content of the data at row 79. We will describe the iloc function later in this lesson.\n\n\\subsubsection{corr()}\nIn Pandas, we can calculate the correlation between different columns of a DataFrame using the .corr() function. Correlation measures how two sets of data relate to each other. Specifically, it quantifies the strength and direction of a relationship between two variables. Let's break it down:\n\\begin{itemize}\n    \\item \\textbf{Correlation Coefficient:} The result of .corr() is a correlation coefficient, typically represented as a value between -1 and 1. The coefficient tells us the degree of association between two variables.\n    \\item \\textbf{Direction:} If the coefficient is positive (closer to 1), it indicates a positive correlation. This means that as one variable increases, the other tends to increase as well. If the coefficient is negative (closer to -1), it signifies a negative correlation, indicating that as one variable goes up, the other tends to go down.\n    \\item \\textbf{Strength:} The closer the coefficient is to -1 or 1, the stronger the relationship. A coefficient near 0 suggests a weak or no correlation, meaning the variables are not significantly related.\n\\end{itemize}\nLet's consider an example on the dataset we are studying.\n\\begin{lstlisting}[language=Python]\ndf.corr()\n\\end{lstlisting}\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.corr.png}\n\\end{figure}\n\n%     Age\t        Education\tUsage\t    Fitness\t    Income\t    Miles\n% Age\t1.000000\t0.277425\t0.013440\t0.059246\t0.511266\t0.034473\n% Education\t0.277425\t1.000000\t0.395155\t0.410581\t0.625827\t0.307284\n% Usage\t0.013440\t0.395155\t1.000000\t0.668606\t0.519537\t0.759130\n% Fitness\t0.059246\t0.410581\t0.668606\t1.000000\t0.535005\t0.785702\n% Income\t0.511266\t0.625827\t0.519537\t0.535005\t1.000000\t0.543473\n% Miles\t0.034473\t0.307284\t0.759130\t0.785702\t0.543473\t1.000000\n\nCalculating the correlation values between different variable/features in the dataset is useful because it helps us understand if there's a connection between two variables. It's commonly used for data analysis to identify patterns and perform cleaning/data processing. In our dataset, here is the output of \\textit{df.corr()}:\n\nNote: We can also compute the correlation between two columns. Here is an example where we compute the correlation between the Age and the Education:\n\\begin{lstlisting}[language=Python]\ndf['Age'].corr(df['Education'])\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\n0.277425\n\\end{lstlisting}\n\n\n\\subsubsection{Data Types:}\nThe \\textit{.info()} method provides an overview of our data, including the data types of each column, the number of non-null entries, and the memory usage. This is helpful for understanding the types of data stored in our DataFrame and for identifying potential missing values.\n\\begin{lstlisting}[language=Python]\ndf.info(show_counts=True, memory_usage=True, verbose=True)\n\\end{lstlisting}\n\nThe given parameters in this \\textit{df.info()} function call are optional. For example, \\textit{show\\_count=True} displays the total number of elements in each column.\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.5\\textwidth]{Images/df.info.png}\n\\end{figure}\n\n% <class 'pandas.core.frame.DataFrame'>\n% RangeIndex: 180 entries, 0 to 179\n% Data columns (total 9 columns):\n%  #   Column         Non-Null Count  Dtype \n% ---  ------         --------------  ----- \n%  0   Product        180 non-null    object\n%  1   Age            180 non-null    int64 \n%  2   Gender         180 non-null    object\n%  3   Education      180 non-null    int64 \n%  4   MaritalStatus  180 non-null    object\n%  5   Usage          180 non-null    int64 \n%  6   Fitness        180 non-null    int64 \n%  7   Income         180 non-null    int64 \n%  8   Miles          180 non-null    int64 \n% dtypes: int64(6), object(3)\n% memory usage: 12.8+ KB\n\nAs you can see in the output, we can see the name of each column in the dataset. In addition, we can see the Non-Null count, meaning the number of data that exist in each column. If there is any missing value in this column, this count will decrease. Furthermore, we can see in the output the Dtype of datatype of each column.  \n\n\\subsubsection{Number of Rows and Columns:}\nThe \\textit{.shape} attribute of the DataFrame allows us to determine the number of rows and columns. Accessing the \\textit{.shape} attribute provides a tuple with the first value representing the number of rows and the second value representing the number of columns.\n\\begin{lstlisting}[language=Python]\nprint(df.shape)   # Gives the number of rows and columns\nprint(df.shape[0])  # Gives the number of rows\nprint(df.shape[1])  # Gives the number of columns\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\n(180, 9)\n180\n9\n\\end{lstlisting}\n\n\\subsubsection{Column Names:}\nTo obtain the names of the columns in our DataFrame, we can use the \\textit{.columns} attribute. If we wish to convert the column names into a regular list, we can apply the \\textit{list()} function to the result.\n\\begin{lstlisting}[language=Python]\ndf.columns\nlist(df.columns)\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\nIndex(['Product', 'Age', 'Gender', 'Education', 'MaritalStatus', 'Usage', 'Fitness', 'Income', 'Miles'], dtype='object')\n\n['Product',\n 'Age',\n 'Gender',\n 'Education',\n 'MaritalStatus',\n 'Usage',\n 'Fitness',\n 'Income',\n 'Miles']\n\\end{lstlisting}\n\n\\subsubsection{Checking for Missing Values:}\nTo identify missing values within our DataFrame, we can use the .isnull() method. It returns a DataFrame with Boolean values, where True indicates a missing value and False signifies a valid entry.\n\n\\begin{lstlisting}[language=Python]\ndf.isnull()\n\\end{lstlisting}\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.isnull.png}\n\\end{figure}\n\n%   Product\tAge\t    Gender\tEducation\tMaritalStatus\tUsage\tFitness\tIncome\tMiles\n% 0\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n% 1\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n% 2\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n% 3\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n% 4\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n% ...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n% 175\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n% 176\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n% 177\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n% 178\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n% 179\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n\nTo count the number of missing values in each column, we can chain the \\textit{.isnull()} method with \\textit{.sum()}. This provides valuable information about the extent of missing data in our DataFrame.\n\n\\begin{lstlisting}[language=Python]\ndf.isnull().sum()\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\nProduct          0\nAge              0\nGender           0\nEducation        0\nMaritalStatus    0\nUsage            0\nFitness          0\nIncome           0\nMiles            0\ndtype: int64\n\\end{lstlisting}\n\nTo obtain the total number of missing values in the entire DataFrame, we can sum the results of \\textit{.isnull().sum()} using \\textit{.sum()} once more.\n\n\\begin{lstlisting}[language=Python]\ndf.isnull().sum().sum()\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\n0\n\\end{lstlisting}\n\n\\subsection{Data Selection and Slicing}\nLet's dive into the various methods for slicing and extracting data in Pandas DataFrames. These methods allow us to isolate specific rows, columns, or subsets of our data.\n\n\\subsubsection{ Isolating One Column Using [ ]:}\nWe can isolate a single column in a Pandas DataFrame by using square brackets [ ] with the column's name. This creates a new \\textbf{Pandas Series} containing the values from that column. A \\textbf{Pandas Series} is a one-dimensional data structure that can be thought of as a labeled array. By default the labels for the series are the items indices. Let's take an example from the CardioGoodFitness.csv dataset.\n\n\\begin{lstlisting}[language=Python]\nage_column = df['Age']\nprint(age_column)\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\n0      18\n1      19\n2      19\n3      19\n4      20\n       ..\n175    40\n176    42\n177    45\n178    47\n179    48\nName: Age, Length: 180, dtype: int64\n\\end{lstlisting}\n\nThe \\textit{age\\_column} will contain all the values from the \\textit{\"Age\"} column, and it will be of type Pandas Series. You can see on the left of the output, we see numbers from 0 to 179, indicating the label of each entry, thus an index by default.\n\n\\textit{It is also important to menion, that using \\textit{.values} on a Panadas Series would return a NumPy array. To manipulate and analyse the content of the array, we can use what we learned in the previous lesson. Here is an example:}\n\n\\begin{lstlisting}[language=Python]\nage_column_numpy = df['Age'].values\n\\end{lstlisting}\n\n\\subsubsection{Isolating Two or More Columns Using [[ ]]:}\nTo isolate multiple columns, we can use double square brackets [[ ]] with a list of column names. This creates a new DataFrame with only the specified columns.\n\nFor example, if we want to isolate both the \"Age\" and \"Gender\" columns, we can do it like this:\n\\begin{lstlisting}[language=Python]\nage_gender_df = df[['Age', 'Gender']]\nprint(age_gender_df)\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\n     Age  Gender\n0     18    Male\n1     19    Male\n2     19  Female\n3     19    Male\n4     20    Male\n..   ...     ...\n175   40    Male\n176   42    Male\n177   45    Male\n178   47    Male\n179   48    Male\n\n[180 rows x 2 columns]\n\\end{lstlisting}\n\n\\textit{age\\_gender\\_df} will be a DataFrame containing only the \"Age\" and \"Gender\" columns.\n\n\\subsubsection{Isolating One Row Using [ ]:}\nWe can retrieve a single row by providing a boolean series with a single 'True' value. In the following example, we obtain the second row with an index of 1. This is accomplished by using \\textit{.index} to access the row labels of the DataFrame and then generating a one-dimensional Boolean array through the comparison operation. Here is an example:\n\\begin{lstlisting}[language=Python]\ndf[df.index == 1]\n\\end{lstlisting}\nThis code returns the row where the index equals 1.\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.row.png}\n\\end{figure}\n\n% \tProduct\tAge\tGender\tEducation\tMaritalStatus\tUsage\tFitness\tIncome\tMiles\n% 1\tTM195\t19\tMale\t15\tSingle\t2\t3\t31836\t75\n\n\\subsubsection{Isolating two or more rows using [ ] }\nTo select and extract two or more rows from a DataFrame, we can employ the \\textit{.isin()} method as an alternative to the equality (==) operator.\n\nFor instance, in the following code, we extract rows where the index falls within the range from 2 to 9. Here is an example:\n\\begin{lstlisting}[language=Python]\ndf[df.index.isin(range(2, 10))]\n\\end{lstlisting}\n\nThis code retrieves rows that have indices within the specified range, making it a convenient way to filter multiple rows simultaneously.\n\nOutput:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.multi.row.png}\n\\end{figure}\n\n%   Product\tAge\tGender\tEducation\tMaritalStatus\tUsage\tFitness\tIncome\tMiles\n% 2\tTM195\t19\tFemale\t14\tPartnered\t4\t3\t30699\t66\n% 3\tTM195\t19\tMale\t12\tSingle\t3\t3\t32973\t85\n% 4\tTM195\t20\tMale\t13\tPartnered\t4\t2\t35247\t47\n% 5\tTM195\t20\tFemale\t14\tPartnered\t3\t3\t32973\t66\n% 6\tTM195\t21\tFemale\t14\tPartnered\t3\t3\t35247\t75\n% 7\tTM195\t21\tMale\t13\tSingle\t3\t3\t32973\t85\n% 8\tTM195\t21\tMale\t15\tSingle\t5\t4\t35247\t141\n% 9\tTM195\t21\tFemale\t15\tPartnered\t2\t3\t37521\t85\n\n\\subsubsection{loc and iloc in Dataframe}\nWe can access specific rows in our DataFrame using \\textit{.loc[]} and \\textit{.iloc[]} (\"location\" and \"integer location\") by providing labels or numeric positions.\n\nIn the example below, we demonstrate the distinction between these two methods. When using .loc[], the \"1\" represents the row label, while in .iloc[], it signifies the row's position (index = 1). Here are the examples:\n\\begin{lstlisting}[language=Python]\n# Access a row with label 1\ndf.loc[1]\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\nProduct           TM195\nAge                  19\nGender             Male\nEducation            15\nMaritalStatus    Single\nUsage                 2\nFitness               3\nIncome            31836\nMiles                75\nName: 1, dtype: object\n\\end{lstlisting}\n\nHere is another example showing the use of \\textit{iloc[]}:\n\n\\begin{lstlisting}[language=Python]\n# Access the row at position 1\ndf.iloc[1]\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\nProduct           TM195\nAge                  19\nGender             Male\nEducation            15\nMaritalStatus    Single\nUsage                 2\nFitness               3\nIncome            31836\nMiles                75\nName: 1, dtype: object\n\\end{lstlisting}\n\nBoth of the loc and iloc return the same row, knowing that loc is label-based indexing, while iloc is integer-based. Remember the output of the \\textit{df.head()}? the numbers we see on the left side is the label of the row. If the label does not start from zero, them the output of loc[1] and iloc[1] will be different.\n\nWe can also retrieve multiple rows by specifying a range within square brackets:\n\\begin{lstlisting}[language=Python]\n# Access rows from 100 to 110 using labels\ndf.loc[100:110]\n\n# Access rows from 100 to 110 using positions\ndf.iloc[100:110]\n\\end{lstlisting}\n\nBoth of the loc and iloc of [100:110] return the same output below:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.iloc.loc.png}\n\\end{figure}\n\n%       Product\tAge\tGender\tEducation\tMaritalStatus\tUsage\tFitness\tIncome\tMiles\n% 100\tTM498\t25\tFemale\t14\tPartnered\t5\t3\t47754\t106\n% 101\tTM498\t25\tMale\t14\tSingle\t3\t3\t45480\t95\n% 102\tTM498\t25\tFemale\t14\tSingle\t2\t3\t43206\t64\n% 103\tTM498\t25\tMale\t14\tPartnered\t4\t3\t45480\t170\n% 104\tTM498\t25\tMale\t14\tPartnered\t3\t4\t43206\t106\n% 105\tTM498\t25\tMale\t16\tPartnered\t2\t3\t50028\t53\n% 106\tTM498\t25\tFemale\t14\tSingle\t2\t2\t45480\t42\n% 107\tTM498\t25\tMale\t14\tSingle\t4\t3\t48891\t127\n% 108\tTM498\t26\tFemale\t16\tPartnered\t4\t3\t45480\t85\n% 109\tTM498\t26\tFemale\t16\tSingle\t4\t4\t50028\t127\n% 110\tTM498\t26\tMale\t16\tSingle\t4\t3\t51165\t106\n\nFurthermore, we can use lists instead of ranges with \\textit{.loc[]} and \\textit{.iloc[]} to select specific rows:\n\n\\begin{lstlisting}[language=Python]\n# Access rows with labels 100, 200, and 300\ndf.loc[[50, 100, 150]]\n\n# Access rows at positions 100, 200, and 300\ndf.iloc[[50, 100, 150]]\n\\end{lstlisting}\n\nThe output of both expressions is:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.loc.specific.png}\n\\end{figure}\n\n%       Product\tAge\tGender\tEducation\tMaritalStatus\tUsage\tFitness\tIncome\tMiles\n% 50\tTM195\t29\tMale\t18\tPartnered\t3\t3\t68220\t85\n% 100\tTM498\t25\tFemale\t14\tPartnered\t5\t3\t47754\t106\n% 150\tTM798\t25\tMale\t16\tPartnered\t4\t5\t49801\t120\n\nIt is also possible to select specific columns in the dataframe using loc and iloc. It's important to note that when selecting specific columns along with rows, there's a difference between \\textit{.loc[]} and \\textit{.iloc[]}. \\textit{.iloc[]} requires column positions rather than column labels:\n\\begin{lstlisting}[language=Python]\n# Access specific columns for rows 100 to 110 using labels\ndf.loc[100:110, ['Product', 'Age', 'Gender']]\n\n# Access specific columns for rows 100 to 110 using positions\ndf.iloc[100:110, :3]\n\\end{lstlisting}\nBoth expressions would result in the same output below:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.2\\textwidth]{Images/df.loc.column.png}\n\\end{figure}\n\n%       Product\tAge\tGender\n% 100\tTM498\t25\tFemale\n% 101\tTM498\t25\tMale\n% 102\tTM498\t25\tFemale\n% 103\tTM498\t25\tMale\n% 104\tTM498\t25\tMale\n% 105\tTM498\t25\tMale\n% 106\tTM498\t25\tFemale\n% 107\tTM498\t25\tMale\n% 108\tTM498\t26\tFemale\n% 109\tTM498\t26\tFemale\n% 110\tTM498\t26\tMale\n\nWe also can access rows starting from a specific index by specifying the starting row index:\n\\begin{lstlisting}[language=Python]\n# Access rows from index 176 to the end for specific columns\ndf.loc[176:, ['Product', 'Age', 'Gender']]\n\n# Access rows from index 176 to the end for specific columns using positions\ndf.iloc[176:, :3]\n\\end{lstlisting}\n\nBoth expressions result in the same output:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.2\\textwidth]{Images/df.loc.index.png}\n\\end{figure}\n\n%       Product\tAge\tGender\n% 176\tTM798\t42\tMale\n% 177\tTM798\t45\tMale\n% 178\tTM798\t47\tMale\n% 179\tTM798\t48\tMale\n\n\\subsection{Conditional slicing}\nIn Pandas, we can filter and select data based on specified conditions, allowing us to isolate rows and columns that meet specific criteria. This concept of condition-based selection in Pandas connects to the concept of universal functions (UFuncs) we discussed in the NumPy lesson, where operations were applied element-wise to arrays.\n\nLet's dive into the details. Note that we continue using the same CardioGoodFitness.csv dataset in our examples.\n\n\\subsubsection{Selecting Rows Based on a Single Condition:}\nTo filter rows based on a single condition, we can use brackets [ ] and specify the condition within them. For example, the following code selects rows where the \"Age\" column is exactly equal to 23. In this case, instead of using row indices or column names, we use a condition where the column \"Age\" is checked for equality (i.e., df.Age == 23), and only rows that meet this condition are returned.\n\n\\begin{lstlisting}[language=Python]\ndf[df.Age == 23]\n\\end{lstlisting}\n\nThe output would contain all rows in the dataframe where the age of the user is exactly 23:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.condition1.png}\n\\end{figure}\n\n%       Product\tAge\tGender\tEducation\tMaritalStatus\tUsage\tFitness\tIncome\tMiles\n% 14\tTM195\t23\tMale\t16\tPartnered\t3\t1\t38658\t47\n% 15\tTM195\t23\tMale\t16\tPartnered\t3\t3\t40932\t75\n% 16\tTM195\t23\tFemale\t14\tSingle\t2\t3\t34110\t103\n% 17\tTM195\t23\tMale\t16\tPartnered\t4\t3\t39795\t94\n% 18\tTM195\t23\tFemale\t16\tSingle\t4\t3\t38658\t113\n% 19\tTM195\t23\tFemale\t15\tPartnered\t2\t2\t34110\t38\n% 20\tTM195\t23\tMale\t14\tSingle\t4\t3\t38658\t113\n% 21\tTM195\t23\tMale\t16\tSingle\t4\t3\t40932\t94\n% 87\tTM498\t23\tMale\t14\tPartnered\t3\t3\t36384\t95\n% 88\tTM498\t23\tMale\t14\tPartnered\t3\t3\t38658\t85\n% 89\tTM498\t23\tFemale\t16\tSingle\t3\t3\t45480\t95\n% 90\tTM498\t23\tMale\t16\tPartnered\t4\t3\t45480\t127\n% 91\tTM498\t23\tFemale\t16\tPartnered\t3\t2\t43206\t74\n% 92\tTM498\t23\tFemale\t14\tSingle\t3\t2\t40932\t53\n% 93\tTM498\t23\tMale\t16\tPartnered\t3\t3\t45480\t64\n% 143\tTM798\t23\tMale\t16\tSingle\t4\t5\t58516\t140\n% 144\tTM798\t23\tFemale\t18\tSingle\t5\t4\t53536\t100\n% 145\tTM798\t23\tMale\t16\tSingle\t4\t5\t48556\t100\n\n\\subsubsection{Selecting Rows Based on Textual Conditions:}\nWe can also filter rows based on textual conditions, such as selecting all rows where the \"Gender\" is 'Female.' In this scenario, df.Gender selects the \"Gender\" column, and df.Gender == 'Female' creates a Boolean Series that identifies which rows have the \"Gender\" value equal to 'Female.' Finally, the [ ] operator takes a subset of the DataFrame where this Boolean Series is True. Let's consider the following example:\n\\begin{lstlisting}[language=Python]\ndf[df.Gender == 'Female']\n\\end{lstlisting}\n\nThe result would contain all the rows where the gender of the user is Female. Here is the output:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Images/df.conditon.2.png}\n\\end{figure}\n\n%   Product\tAge\tGender\tEducation\tMaritalStatus\tUsage\tFitness\tIncome\tMiles\n% 2\tTM195\t19\tFemale\t14\tPartnered\t4\t3\t30699\t66\n% 5\tTM195\t21\tFemale\t14\tPartnered\t3\t3\t32973\t66\n% 6\tTM195\t21\tFemale\t14\tPartnered\t3\t3\t35247\t75\n% 9\tTM195\t21\tFemale\t15\tPartnered\t2\t3\t37521\t85\n% 11\tTM195\t22\tFemale\t14\tPartnered\t3\t2\t35247\t66\n% ...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n% 152\tTM798\t25\tFemale\t18\tPartnered\t5\t5\t61006\t200\n% 157\tTM798\t26\tFemale\t21\tSingle\t4\t3\t69721\t100\n% 162\tTM798\t28\tFemale\t18\tPartnered\t6\t5\t92131\t180\n% 167\tTM798\t30\tFemale\t16\tPartnered\t6\t5\t90886\t280\n% 171\tTM798\t33\tFemale\t18\tPartnered\t4\t5\t95866\t200\n\n\\subsubsection{Selecting Specific Columns Based on Conditions:}\nWe can use comparison operators like $>$ to draw comparisons and select specific columns along with rows based on conditions. The following code retrieves the \"Age,\" \"Gender,\" and \"Income\" columns for all records where \"Income\" is greater than 92,000. Using \\textit{.loc[]}, we specify the condition as \\textit{df['Income'] $>$ 92,000}, and then we list the desired columns in the square brackets.\n\\begin{lstlisting}[language=Python]\ndf.loc[df['Income'] > 92000, ['Age', 'Gender', 'Income']]\n\\end{lstlisting}\n\nThe output is:\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[width=.3\\textwidth]{Images/df.select.column.png}\n\\end{figure}\n\n%       Age\tGender\tIncome\n% 162\t28\tFemale\t92131\n% 168\t30\tMale\t103336\n% 169\t30\tMale\t99601\n% 171\t33\tFemale\t95866\n% 172\t34\tMale\t92131\n% 173\t35\tMale\t92131\n% 174\t38\tMale\t104581\n% 178\t47\tMale\t104581\n% 179\t48\tMale\t95508\n\n\\subsection{Pandas Grouping}\nIn Pandas, we can perform advanced data analysis through grouping and aggregation using the \\textit{groupby()} operation. This operation allows us to split, apply, and combine data, which is essential for summarizing information based on specific criteria.\n\n\\subsubsection{Split, Apply, Combine:}\nThink of the \\textit{groupby()} operation as a way to split our data, apply a function to each group, and then combine the results. This concept is crucial for various data analysis tasks. A visual illustration of these operations is presented in Figure \\ref{fig:df_groupby}\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics{Images/pd.grouping.png}\n    \\caption{A representation of the grouping operation}\n    \\label{fig:df_groupby}\n\\end{figure}\n\n\\textbf{Splitting Data:} In the split step, we can break down our DataFrame into groups based on a chosen key or label. This key could be a column in our dataset. For example, we could group data by categories like \"A,\" \"B,\" and \"C\" based on the values in a \"key\" column, as shown in Figure \\ref{fig:df_groupby}.\n\n\\textbf{Applying a Function:} After splitting the data, we can apply various functions or operations to each group. This could include computing sums, averages, counts, minimums, maximums, or custom operations.\n\n\\textbf{Combining Results:} Finally, the groupby() operation combines the results of the individual groups, giving us a summary of our data based on the applied function.\n\n\\subsubsection{Example:}\nLet's say we have a DataFrame with columns \"key\" and \"data\" like this (it is slighlty different from the one shown in Figure \\ref{fig:df_groupby}):\n\\begin{lstlisting}[language=Python]\nkey   data\n A     0\n B     1\n C     2\n A     3\n B     4\n C     5\n\\end{lstlisting}\n\n\\begin{itemize}\n    \\item We can group this data by the \"key\" column using \\textit{df.groupby('key')}.\n    \\item To apply an operation, such as finding the sum for each group, we can use \\textit{df.groupby('key').sum()}.\n    \\item The result will show the sum for each group:\n    \\begin{lstlisting}[language=Python]\n    key   data\n    A     3\n    B     5\n    C     7\n    \\end{lstlisting}\n\\end{itemize}\n\n\\subsubsection{GroupBy Object:}\nThe \\textit{groupby()} operation returns a DataFrameGroupBy object. This object is like a special lens through which we can view and manipulate the grouped data.\n\n\\textbf{Aggregate, Filter, Transform, Apply:} The GroupBy object provides powerful functions for aggregation, filtering, transforming, and applying operations to the grouped data. We can perform various tasks on the grouped data, such as finding filtering based on criteria, applying custom functions, and more. Here is an example:\n\\begin{lstlisting}[language=Python]\n# Calculate the sum of values in each group\nresult = df.groupby('key')['data'].agg(['sum', 'mean'])\nresults\n\\end{lstlisting}\n\nHere's what the result variable contains based on the previous data:\n\\begin{lstlisting}[language=Python]\n     sum  mean\nkey\nA      3   1.5\nB      5   2.5\nC      7   3.5\n\\end{lstlisting}\nThe results is a DataFrame where each row represents a unique value in the \"key\" column, and the \"sum\" and \"mean\" columns show the aggregated values for each group.\n\n\\textbf{Iteration Over Groups:} We can iterate directly over the groups created by \\textit{groupby()}, making it possible to manually perform tasks on each group. Here is an example:\n\\begin{lstlisting}[language=Python]\n# Manually iterate over groups and print group information\nfor key, group in df.groupby('key'):\n    print(f\"Group '{key}' has {group.shape[0]} rows.\")\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}[language=Python]\nGroup 'A' has 2 rows.\nGroup 'B' has 2 rows.\nGroup 'C' has 2 rows.\n\\end{lstlisting}\n\nThe code example iterates through each group based on the unique values in the \"key\" column and prints the group label and the number of rows in each group.\n\n\\subsection{Summary}\nIn conclusion, we have enhanced our data analysis skills by delving into the power of Pandas. In our previous lesson, we covered NumPy, and now, with Pandas, we are better equipped to efficiently handle and analyze data. We have built strong skills for exploratory data analysis. \n\nThroughout this lesson, we've explored the core concepts of Pandas DataFrames and Series, gaining a better understanding of their properties and functionalities. We've learned how to load data, visualize and interpret it, extract valuable statistical information, and use selection, slicing, and grouping techniques.\n\nAs we apply these newfound skills, we have the CardioGoodFitness.csv dataset as examples. In the practical exercise, we will be practicing on our well-know Heart Failure Prediction dataset. This real-world dataset will serve as an excellent platform to put our knowledge to work.\n\n\n\n\\subsection{Practical Exercises: Pandas [60 mins]}\nThe purpose of this exercise is to apply what we have learned about pandas on the Heart Failure Prediction dataset. You can find the csv file of the dataset (heart.csv) on Github under Week4. \n\n\\textbf{Instructions:}\n\\begin{enumerate}\n    \\item Download the Pandas Exercises notebook file\n    \\item Follow the headers in the notebook to write the code\n    \\item Remember that you can go back and visit the content of the week. Pandas functions and properties described in lesson 2 are very helpful.\n\\end{enumerate}\n\n\\subsubsection*{Guidelines}\n[TO BE ADDED BY LD]\n\n\\subsection{Model Answer(s): Pandas }\n[Page content]\n\n\\subsection{Share and Compare: Pandas [30 mins] }\n\\subsubsection*{Your task}\n[short introduction to the activity, followed by a succinct explanation of the task(s)]\n\n\\subsubsection*{Guidelines}\n[TO BE ADDED BY LD]\n\n\n%", "metadata": {"source": "./full-course/aai.txt"}}}