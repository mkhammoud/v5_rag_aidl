{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "{Lesson 2: Scikit-Learn \\& Logistic Regression}\n\n\\subsection{Introduction}\nIn this lesson, we are going to explore together one of the most popular Machine Learning (ML) libraries, Scikit-Learn. This library helps us build ML solutions without having to worry about the underlying mechanisms of model building, training, and prediction. With a wide variety of available ML models, we can play with different ones. All we have to do is simply follow an Application Programming Interface (API) to build the ML solution. Afterward, we start building ML solutions by following the API on a sample dataset using logistic regressions. We present a step-by-step guide on how to build a logistic regression model using Scikit-Learn for the Heart-Failure Prediction dataset.\n\n\\subsection{Introducing Scikit-Learn}\nScikit-Learn is one of the most widely-used Python libraries for machine learning. Known for its efficiency and extensive implementations of common algorithms, Scikit-Learn offers a clean, uniform API that simplifies the learning curve. This section offers an overview of the API's key elements, providing a solid foundation to building machine learning algorithms. We will be dicovering together Linear Regression and Logistic Regression as the first basic models to learn. Knowing how to deal with these models will let us build other ones with almost similar steps.\n\n\\subsubsection{Data Representation in Scikit-Learn}\nBuilding a Machine Learning solution is all about the data that we have. Therefore, we are going to explain how to represent data in Scikit-Learn, to be fed for the model. Think of it simply as a table-structure information.\n\n\\subsubsection*{Data as table}\nA basic table is simply a two-dimensional grid of information, which is visually divided into rows and columns. Each row is an element in the dataset, while each column is a feature or quantity representation of each of these elements. Let us take the heart disease dataset as a use case, and load its rows using Pandas.\n\n\\begin{lstlisting}[language=Python]\nimport numpy as np\nimport pandas as pd\ndata=pd.read_csv('./heart.csv')\ndata.head()\n\\end{lstlisting}\n\nOutput: %List the below output in table format\n% \tsepal_length\tsepal_width\tpetal_length\tpetal_width\tspecies\n% 0\t5.1\t3.5\t1.4\t0.2\tsetosa\n% 1\t4.9\t3.0\t1.4\t0.2\tsetosa\n% 2\t4.7\t3.2\t1.3\t0.2\tsetosa\n% 3\t4.6\t3.1\t1.5\t0.2\tsetosa\n% 4\t5.0\t3.6\t1.4\t0.2\tsetosa\n\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\# & Age & Sex & ChestPainType & RestingBP & Cholesterol & FastingBS & RestingECG & MaxHR & ExerciseAngina & Oldpeak & ST\\_Slope & HeartDisease \\\\\n\\hline\n0 & 40 & M & ATA & 140 & 289 & 0 & Normal & 172 & N & 0.0 & Up & 0 \\\\\n1 & 49 & F & NAP & 160 & 180 & 0 & Normal & 156 & N & 1.0 & Flat & 1 \\\\\n2 & 37 & M & ATA & 130 & 283 & 0 & ST & 98 & N & 0.0 & Up & 0 \\\\\n3 & 48 & F & ASY & 138 & 214 & 0 & Normal & 108 & Y & 1.5 & Flat & 1 \\\\\n4 & 54 & M & NAP & 150 & 195 & 0 & Normal & 122 & N & 0.0 & Up & 0 \\\\\n\\hline\n\\end{tabular}\n\n\n\nHere each row in the dataset represents a patient record. We will refer to the rows as \\textit{samples} and the number of rows as \\textit{n\\_samples}.\n\nLikewise, each column represents an information for each of the patients describing it. We will refer to the columns as \\textit{features} and for the number of columns as \\textit{n\\_features}. As we saw in previous weeks, features can either take to form of role values or other types, such as boolean, string, etc.\n\n\\subsubsection*{Features matrix}\nAs we discussed, the data can be represented as a two-dimensional table. In terms of arrays, we can represent the data as a two-dimensional array. By convention, this feature matrix is represented as \\textit{X} and is assumed to be two-dimensional, with shape \\textit{[nb\\_sample, nb\\_features]}. This array is mostly contained in a NumPy array or Pandas Dataframe (discussed in Week 6). \n\n\\subsubsection*{Target array}\nIn addition to the features arrary \\textit{X}, we work with labels or target array, which by convention we refer to as \\textit{y}. The target array is usually single-dimensional and has the size of \\textit{nb\\_samples}. This target array can be of continuous values (numbers) or discrete (classes/labels). \n\nA common confusion is how the target array is different from the features or columns we have in the features matrix. The sample answer is that the target array is the value we want to predict. In the heart disease dataset example, we want to predict whether the patient is diseased or not, thus the target array is \\textit{HeartDisease}. The rest of the columns form the features matrix. \n\nKnowing the target array, we can use seaborn to visualize the data. Thus we specify \\textit{HeartDisease} as the value for parameter \\textit{hue}.\n\n\\begin{lstlisting}[language=Python]\nimport seaborn as sns\nsns.set()\nsns.pairplot(data, hue='HeartDisease')\n\\end{lstlisting}\n\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[scale=0.4]{Images/sns.png}\n\\end{figure}\n\nNow, we will extract the features matrix and target array from the dataframe. We will use some operations discussed earlier in Week 6.\n\\begin{lstlisting}[language=Python]\nX_data = data.drop('HeartDisease', axis=1)\nX_data.shape\n\\end{lstlisting}\n\n\\begin{lstlisting}[language=Python]\ny_data = data['HeartDisease']\ny_data.shape\n\\end{lstlisting}\n\nWhat we did was extract the features matrix by dropping it from the dataframe. Furthermore, we extracted the target array by specifying the column name \\textit{HeartDisease}. Here is a summary of what we are trying to achieve:\n\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[scale=0.4]{Images/data_layout.png}\n\\end{figure}\n\nNow, let's consider the Scikit-Learn API or steps needed to build the ML model.\n\n\\subsection{Scikit-Learn API}\nThe steps for using the Scikit-Learn API are listed below:\n\\begin{enumerate}\n    \\item Pick a type of model by bringing in the right class from Scikit-Learn.\n    \\item Set the model's settings by creating an instance of the class with your preferred values.\n    \\item Organize your data into a features table and target list.\n    \\item Train the model with your data by using the \\textit{\\textbf{fit()}} method on the model instance.\n    \\item Use the trained model on new data. We predict labels for unknown data using the \\textit{\\textbf{predict()}} method.\n\\end{enumerate}\n\nLet's apply these steps on simple Machine Learning models.\n\n% \\subsubsection{Basics of the API}\n\n\\subsection{Logistic Regression}\nLogistic regression is a statistical method used for binary classification, where the aim is to predict the probability of a particular outcome. For instance, if we are dealing with a certain application like the heart disease dataset, Logistic Regression will be able to tell what is the probability of a certain patient being diseased or not.\nLogistic regression is widely used due to its simplicity, efficiency, and interpretability. It's commonly used in various fields such as healthcare for disease prediction, finance for credit scoring, and in many other domains where binary classification is needed.\n\n\\textbf{\\textit{Note}}: In this week, we also explain the Linear Regression model in detail, including its equation. Both linear and logistic regressions utilize similar underlying principles. They both employ a linear equation (with coefficients and variables) to make predictions. However, the main difference lies in how they handle the output. It is worth noting that we \\textbf{don't} have to go through the mathematical formulas or any sort of validations of ML for this course. Feel free to explore the content in the extra lesson if you feel like it.\n\n\nFrom this section onward, we will assume that we are interested in devising a Logistic Regression model to be able to predict whether a patient is diseased or not.\nThe code for this part can be found on Github.\n\n\nYou can also follow the videos for a detailed explanation of the three phases:\nLAU AAI2 W8 L2 Logistic Regression - Phase 1\nLAU AAI2 W8 L2 Logistic Regression - Phase 2\nLAU AAI2 W8 L2 Logistic Regression - Phase 3\n\\includegraphics[width=\\textwidth]{Images/video_resource.png} \n\n\nTo begin with, we will import our dataset. (Do not forget to have the heartdisease dataset accessible for the following notebook.\n\n\\begin{lstlisting}[language=Python]\n# Reading the dataset.\nimport pandas as pd\ndata=pd.read_csv('./heart.csv')\n\\end{lstlisting}\n\nWe conduct the dataset cleaning and preprocessing stages first before we move into the ML parts. We addressed this part in the previous week, you can refer back to it for more information.\n\nThe first code snippet aims to eliminate duplicate rows within the DataFrame data. Using the drop\\_duplicates() method, it creates a new DataFrame named data\\_drop.\n\\begin{lstlisting}[language=Python]\n# Removing duplicates\ndata_drop = data.drop_duplicates()\n\\end{lstlisting}\n\nEmploying pd.get\\_dummies(), this process transforms categorical columns such as 'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', and 'ST\\_Slope' into binary columns, generating indicator variables for each category\n\\begin{lstlisting}[language=Python]\n# Encoding dataset\ndata_enc = pd.get_dummies(data_drop, columns=['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'])\n\\end{lstlisting}\n\nThen, we utilize MinMaxScaler() from Scikit-Learn to rescale numerical features within a specified range, commonly 0 to 1, ensuring consistent scaling among these attributes.\n\\begin{lstlisting}[language=Python]\n# Normalize the columns\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data_enc), columns=data_enc.columns)\n\\end{lstlisting}\n\n\nBefore we start, download Scikit-Learn if you don't have it installed using the below. Paste the below code in a notebook cell and run it.\n\\begin{lstlisting}[language=Python]\n!pip install -U scikit-learn\n\\end{lstlisting}\n\n\\textbf{\\textit{Now let's use the recipe outlined earlier to build the Logistic Regression model.}}\n\n\\subsubsection*{Choose a class of model.}\nIn Scikit-Learn, every ML model is represented by a Python class (Remember Classes discussed in Week 5). Therefore, to build a Linear Regression model, we would have to import it as follows:\n\\begin{lstlisting}[language=Python]\nfrom sklearn.linear_model import LogisticRegression\n\\end{lstlisting}\n\nThe dataset earmarked for training is split into two subsets: the training set and the testing set. The rationale behind this division primarily lies in training the AI model with specific data and subsequently assessing its proficiency by testing it with previously unseen data. To familiarize ourselves more with this concept, imagine you being a teacher and you want to give an exam to students. You have a pool of 10 programming questions. What you do, is you teach the students about 7 of these 10 questions and then you examine them with the remaining 3 to see if they understood the class correctly.\n\nIn our context, we can divide the data we used for training (we refer to as \\textit{train\\_data}) into two parts. We pass the first part for training, while we keep the second part to test the performance of the model. We refer to the test data as \\textit{test\\_data}. By convention, we divide the data into 80\\% training and 20\\% for testing. There is a nice function in Scikit-Learn to do that, which is \\textit{train\\_test\\_split}.\n\nTo split, we apply the following method and parameters to have 80\\% of the data as a training set and 20\\% of the data as a testing set:\n\n\\begin{lstlisting}[language=Python]\nlabels_data = data_scaled['HeartDisease']\nfeatures_data = data_scaled.drop('HeartDisease', axis=1)\n\n\nfrom sklearn.model_selection import train_test_split\n\n#split the X and y into train and test (0.2 refers to the 20\\%)\nX_train, X_test, y_train, y_test = train_test_split(features_data,labels_data, test_size = 0.2, random_state = 42)\n\n#printing the data\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\\end{lstlisting}\n\n\nWe initialize a logistic regression classifier using Scikit-Learn's LogisticRegression() class after the necessary library has been imported (as denoted in the preceding code). It trains the classifier using the fit() method by passing the training data (X\\_train - features, and y\\_train - target) to the model. This step involves the learning process where the classifier adjusts its parameters to find the best decision boundary separating different classes based on the provided training data.\n\n\\begin{lstlisting}[language=Python]\n# after importing the library (two codes above)\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\n\\end{lstlisting}\n\nAfterward, we test our AI model by predicting the X\\_test rows that we stored from the code above. Then, we use the accuracy\\_score function to measure the correct prediction percentage.\n\n\\begin{lstlisting}[language=Python]\nfrom sklearn.metrics import accuracy_score\n\ny_pred = classifier.predict(X_test)\n    \n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model: Logistic Regression\")\nprint(f\"Accuracy: {accuracy:.2f}\")\n\\end{lstlisting}\n\nOutput:\n\n\\begin{lstlisting}\nModel: Logistic Regression\nAccuracy: 0.86\n\\end{lstlisting}\n\nNotice that you might get different training results depending on many factors.\n\n\\subsubsection*{Predict labels for unknown data.}\nOnce the model is trained, we can start performing some prediction. This is why we are learning about Machine Learning, right? We want to pass the model data and see what decision it will make. We can do this using the \\textit{predict()} function of the Linear Regression class.\n\nLet's create some data to perform prediction on. Let us assume that we want to predict the HeartDisease status of this new record with the following features:\n\n\\begin{lstlisting}\n    Age: 40,\n    Sex: 'M',\n    ChestPainType: 'ATA',\n    RestingBP: 140,\n    Cholesterol: 289,\n    FastingBS: 0,\n    RestingECG: 'Normal',\n    MaxHR: 172,\n    ExerciseAngina: 'N',\n    Oldpeak: 0.0,\n    ST_Slope: 'Up'\n\\end{lstlisting}\n\n\\textbf{As we did before, we have to reshape the data before passing it to the model for prediction. Remember this step very well, because if we do any kind of preprocessing to the data before we pass it for training, we have to repeat that again before prediction}. For simplicity, we will create a function that converts a given new record to a predictable record.\n\n\\begin{lstlisting}[language=Python]\n# receive a dictionary as input\ndef prepare_prediction_row(data_pred):\n    data_pred = pd.DataFrame(data_pred)\n    # merge the new data with all the dataset\n    new_data = pd.concat([data, data_pred], ignore_index=False)\n    # NOW, WE REPEAT THE SAME PREPROCESSING STEPS\n    # encoding\n    data_pred_encoded = pd.get_dummies(new_data, columns=['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'])\n    # scaling\n    data_test_scaled = pd.DataFrame(scaler.transform(data_pred_encoded), columns=data_pred_encoded.columns)\n    # store the newly added row in a separate variable\n    row_to_predict = data_test_scaled.tail(1)\n    # remove the label column that was automatically added to the row \n    row_to_predict = row_to_predict.drop('HeartDisease', axis=1)\n    # return the row\n    return row_to_predict\n\\end{lstlisting}\n\nFor more elaboration on this function:\n\\begin{itemize}\n    \\item DataFrame Initialization: The function takes Dictionary data\\_pred as input, presumably representing the new record to be predicted. It converts this data into a Pandas DataFrame named data\\_pred.\n\n    \\item Merging Data: Next, the function combines the new data (data\\_pred) with the entire existing dataset (data). This concatenation is performed using pd.concat() to ensure that the preprocessing steps are consistent and applied uniformly to the entire dataset and the new record. The parameter ignore\\_index=False retains the original indices.\n    \n    \\item Preprocessing Steps: After merging, the function re-executes the preprocessing steps previously applied to the dataset. It encodes categorical columns using one-hot encoding (pd.get\\_dummies()) for columns like 'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', and 'ST\\_Slope'. Following this, it scales the data using the previously defined MinMaxScaler().\n    \n    \\item Isolating New Row: The function then isolates the newly added row (which represents the new record) from the merged and preprocessed dataset (data\\_test\\_scaled). It uses tail(1) to extract the last row (which corresponds to the new record) and stores it in row\\_to\\_predict. \n\n    \\item Removing Label Column: The new record, obtained in row\\_to\\_predict, may have a label column (HeartDisease) that was concatenated during the preprocessing. As we're preparing this row for prediction, it removes the label column using drop() with axis=1 to specify that it's dropping a column.\n    \n    \\item Return Statement: Finally, the function returns the preprocessed and formatted row (row\\_to\\_predict), ready to be used for predictions by the logistic regression model.\n\\end{itemize}\n\n\nWe can now pass the data to the model for prediction after passing it to the prepare\\_prediction\\_row function first.\n\n\\begin{lstlisting}[language=Python]\n# Example of row transformation using the function\ndata_pred = {'Age': [40],\n             'Sex': ['M'],\n             'ChestPainType': ['ATA'],\n            'RestingBP': [140],\n            'Cholesterol': [289],\n            'FastingBS': [0],\n            'RestingECG': ['Normal'],\n            'MaxHR': [172],\n            'ExerciseAngina': ['N'],\n            'Oldpeak': [0.0],\n            'ST_Slope': ['Up']\n            }\nrow_to_predict = prepare_prediction_row(data_pred)\n\\end{lstlisting}\n\nPredicting the value of the new record can be done using the 'predict' function of the model. In the following code, we stored the prediction in the variable is\\_patient\\_heart\\_diseased. It should come either 0 or 1. However, for clarity, we convert it to boolean when we print it.\n\\begin{lstlisting}[language=Python]\n# Predicting the row\nis_patient_heart_diseased = classifier.predict(row_to_predict)\nprint(\"The new patient's heart disease condition is\", bool(is_patient_heart_diseased))\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}\nThe new patient's heart disease condition is False\n\\end{lstlisting}\n\nWe can also go one step further as the logistic model can decide whether a record is diseased or not according to a certain percentage. We can tell Python to show us this percentage to see the probability of being certain about his decision:\n\n\\begin{lstlisting}[language=Python]\n# Checking the probability of the prediction\n# By predict probabilities of each class\nprobabilities = classifier.predict_proba(row_to_predict)\n# Displaying the probabilities\nprint(\"Probabilities for each class:\", probabilities)\n\\end{lstlisting}\n\nOutput:\n\\begin{lstlisting}\nProbabilities for each class: [[0.95938701 0.04061299]]\n\\end{lstlisting}\n\nBy reading the probabilities for the patient being Negative (first number) or Positive (second number), we notice it is highly negative with almost 95\\% (0.95938701). Therefore, the prediction in the previous code showed us that the value is False, and now we know how sure the model is of his prediction.\n\n\n\n\\subsection{Share and Compare: Logistic Regression}\n\\subsubsection*{Your task}\nWe invite you to engage in active learning by participating in an interactive discussion. This is a wonderful opportunity to share your insights, questions, and challenges related to the Machine Learning and Logistic Regression topic.\nIt is worth noting that we did not dive deep into the core concepts of each model, however, we were only focusing on a wider scope to be able to comprehend the ML process as a sequence. Almost all of the AI problems with similar problem formulations can be resolved in the same manner. Even though it is not required, feel free to work on an external dataset after completing this course (Some of the main sources to download datasets from can be Kaggle website). Because the best way to learn programming is, again, to practice.\n\nEngage in a group discussion where each participant shares their understanding of the key concepts learned in this lesson. What was the most difficult part about this lesson? And how challenging was it to solve your first AI problem? \n\\subsubsection*{Guidelines}\n[TO BE ADDED BY LD]\n\n%", "metadata": {"source": "./full-course/aai.txt"}}}