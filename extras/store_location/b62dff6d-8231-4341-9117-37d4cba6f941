{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "{Lesson 2: Data Cleaning \\& Preprocessing [120 mins]}\n\\subsection{Introduction}\nDatasets are prone to errors due to factors like human mistakes in input, equipment malfunctions, and inconsistencies across recording methods. Such a problem can degrade the learning quality of machines when conducting machine learning processes. For instance, imagine if you have a book full of errors and you tell a student to study such a book. The outcome won't be desirable. To overcome such a limitation, Data Cleaning and Preprocessing are usually conducted. Both of these processes are fundamental steps in refining raw data before employing machine learning algorithms or statistical analyses. The objective is to ready the data for analysis or modeling by enhancing its quality and usability, thereby improving the predictive models' accuracy and efficiency. In the next sections, we will elaborate on these tasks and discuss in a summarized manner what is happening in each of them. We provide in this context a modified version of the heart disease dataset. You can also find it on Github.\n\n\n%Data cleaning tackles tasks like handling missing data, rectifying inaccuracies, eliminating duplicates, and managing outliers. On the other hand, data preprocessing focuses on transforming raw data into a usable format for machine learning models. It encompasses actions such as normalization, scaling, encoding categorical variables, selecting pertinent features, and addressing imbalances in data distribution. \n\n\n\n\n\\subsection{Data Cleaning}\nThe process of data cleaning involves identifying and rectifying errors, inconsistencies, or inaccuracies in the dataset. As was previously mentioned, collected data are prone to errors. Therefore, it is quite essential to clean the data before conducting any learning processes.\n\n%Tasks include handling missing values, correcting erroneous entries, removing duplicate records, and addressing outliers or anomalies. Cleaning ensures that the data is accurate, complete, and ready for further analysis.\n\\subsubsection{Handling Missing Values}\nOften, collected datasets might contain empty cells. You were conducting a study and you gave a survey to your neighbor to fill out. He filled everything in that survey except for his age which he forgot to enter. Machine Learning might take it personally; Null values in a dataset can significantly impact machine learning models. Depending on the algorithm used, null values might cause errors during model training or prediction stages. Most machine learning algorithms cannot handle null values directly and might either produce errors or produce biased results if null values are not addressed beforehand. Thus, handling null values is crucial during the first stage to ensure the model's accuracy and performance. \n\n\\textbf{\\textit{Detection Mechanism:}}\nTo detect whether we have missing values in the dataset or not, we can simply apply the \\textit{isnull()} method like the following:\n\\begin{lstlisting}[language=Python]\n    # import the modified dataset\n    import pandas as pd\n    data = pd.read_csv('.\\heart_modified.csv')\n    \n    # Let us print the sum of the null values\n    print(data.isnull().sum())\n\\end{lstlisting}\nThe output will look something like this:\n\\begin{lstlisting}\n    Age               6\n    Sex               0\n    ChestPainType     0\n    RestingBP         0\n    Cholesterol       0\n    FastingBS         0\n    RestingECG        0\n    MaxHR             0\n    ExerciseAngina    0\n    Oldpeak           0\n    ST_Slope          0\n    HeartDisease      0\n    dtype: int64\n\\end{lstlisting}\nAccording to the above, it seems that the Age column has 6 missing variables in the whole dataset. \n\n\\textbf{\\textit{Handling Mechanism:}}\nAside from asking your neighbor to fill in the empty value, there are several strategies to deal with such a scenario. Below we address some of the techniques.\n\\begin{itemize}\n    \\item \\textbf{Dropping}: A possible way to fix the problem is to delete the problem. Now it might sound extreme, but in a lot of cases, the dataset will not be affected by removing the 'few' null records.\n    The code below can perform this task:\n    \\begin{lstlisting}[language=Python]\n        data = data.dropna()\n    \\end{lstlisting}\n    \\textit{dropna} is a function that drops the rows having 'not available \\textit{na}' values.\n\n    \\item \\textbf{Imputation}: Another way is to fill the missing values with a specific value. We can utilize the \\textit{mean} for example to fill the empty cells.\n    \\begin{lstlisting}[language=Python]\n        data['age'] = data['age'].fillna(data['age'].mean())\n    \\end{lstlisting}\n    As can be noticed, \\textit{fillna} is used here to fill all of the empty cells with the mean variable of that column.\n\n    \\item \\textit{Advanced Methods}: Some other methods can include the utilization of Machine Learning to predict the missing values based on other features in the datasets. Others can consist of more complicated imputation techniques such as the utilization of K-nearest neighbors (KNN) imputation, interpolation, or using models specifically designed for handling missing values. Do not worry about this part, it is advanced for this course.\n\\end{itemize}\n\nAfter applying any of the above methods, the null problem should be resolved.\nIf we execute the following command again:\n\\begin{lstlisting}[language=Python]\n    # Assuming that we already imported a dataset (any) \n    # Let us print the sum of the null values\n    print(data.isnull().sum())\n\\end{lstlisting}\nThe output will show 0's in every column:\n\\begin{lstlisting}\n    Age               0\n    Sex               0\n    ChestPainType     0\n    RestingBP         0\n    Cholesterol       0\n    FastingBS         0\n    RestingECG        0\n    MaxHR             0\n    ExerciseAngina    0\n    Oldpeak           0\n    ST_Slope          0\n    HeartDisease      0\n    dtype: int64\n\\end{lstlisting}\n\n\\subsubsection{Handling Duplicate Records}\nDuplicate records, if they're exact duplicates, can often be removed without impacting the quality of our machine learning model. They don't contribute additional information and might affect the analysis or model training.\n\n\\textbf{\\textit{Detection Mechanism:}}\nTo detect whether we have duplicate values in the dataset or not, we can simply apply the \\textit{duplicated()} method like the following:\n\\begin{lstlisting}[language=Python]\n    print(data.duplicated().sum())\n\\end{lstlisting}\nUpon execution, this code will display the count of duplicate records within the dataset. The output after applying it to the modified dataset is:\n\\begin{lstlisting}\n    4\n\\end{lstlisting}\nThere seem to be 4 duplicated records in the dataset.\n\n\\textbf{\\textit{Handling Mechanism:}}\nIn a similar manner, there are several strategies to deal with such a scenario. Below we address some of the techniques.\n\\begin{itemize}\n    \\item \\textbf{Dropping Duplicates:} Removing duplicate records from the dataset is a common approach to mitigate their impact. This operation can be performed using Pandas' drop\\_duplicates() method:\n\\begin{lstlisting}[language=Python]\n    data = data.drop_duplicates()\n\\end{lstlisting}\nAfter executing the above command, we can test it again using the duplicated() method:\n\\begin{lstlisting}[language=Python]\n    print(data.duplicated().sum())\n    # output 0\n\\end{lstlisting}\n\n\\item \\textbf{Preserving Information:} In certain cases, rather than discarding duplicates entirely, it might be beneficial to keep track of their occurrence, especially when dealing with time series data or scenarios where duplicates hold unique identifiers or timestamps.\n\\end{itemize}\nAfter we drop the duplicates, if we rerun the detection mechanism again, it will show 0.\n\n\\subsection{Data Preprocessing}\nData preprocessing focuses on transforming raw data into a usable format for machine learning models. It encompasses actions such as normalization, scaling, encoding categorical variables, and selecting pertinent features.\n\n\\subsubsection{Encoding Categorical Variables}\nEncoding is necessary when dealing with categorical variables in your dataset. Machine learning models often work with numerical data, and categorical variables\u2014like \"gender,\" \"city,\" or \"type of car\"\u2014are not inherently numerical. Encoding involves converting these categorical variables into a numerical format that the machine learning algorithms can process effectively. There are various encoding techniques such as label encoding, one-hot encoding, and target encoding, each with its own benefits and suitable use cases.\nIn this lesson, we will focus on One Hot Encoding. This method will create new columns for each unique category in the columns, assigning binary values (0 or 1) to indicate the presence of a specific category.\nAn example can be seen below:\n\n\\begin{lstlisting}[language=Python]\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\ndata = pd.read_csv('.\\heart.csv')\ndata.head()\n\\end{lstlisting}\n\nThis code will show the following:\n\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Age} & \\text{Sex} & \\text{ChestPainType} & \\text{RestingBP} & \\text{Cholesterol} & \\text{FastingBS} & \\text{RestingECG} & \\text{MaxHR} & \\text{ExerciseAngina} & \\text{Oldpeak} & \\text{ST\\_Slope} & \\text{HeartDisease} \\\\\n\\hline\n40 & \\text{M} & \\text{ATA} & 140 & 289 & 0 & \\text{Normal} & 172 & \\text{N} & 0.0 & \\text{Up} & 0 \\\\\n49 & \\text{F} & \\text{NAP} & 160 & 180 & 0 & \\text{Normal} & 156 & \\text{N} & 1.0 & \\text{Flat} & 1 \\\\\n37 & \\text{M} & \\text{ATA} & 130 & 283 & 0 & \\text{ST} & 98 & \\text{N} & 0.0 & \\text{Up} & 0 \\\\\n48 & \\text{F} & \\text{ASY} & 138 & 214 & 0 & \\text{Normal} & 108 & \\text{Y} & 1.5 & \\text{Flat} & 1 \\\\\n54 & \\text{M} & \\text{NAP} & 150 & 195 & 0 & \\text{Normal} & 122 & \\text{N} & 0.0 & \\text{Up} & 0 \\\\\n\\hline\n\\end{array}\n\nWe notice that the column \"Sex\" for example is presented as a character. This is not easy for machine learning models to deal with. Therefore, we apply One Hot Encoding:\n\n\\begin{lstlisting}[language=Python]\n#apply one hot encoding for every non-numerical column in the dataset\ndata = pd.get_dummies(data)\ndata.head()\n\\end{lstlisting}\n\nThe new table will look like the following:\n\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Age} & \\text{RestingBP} & \\text{Cholesterol} & \\text{FastingBS} & \\text{MaxHR} & \\text{Oldpeak} & \\text{HeartDisease} & \\text{Sex\\_F} & \\text{Sex\\_M} & \\text{ChestPainType\\_ASY} & \\ldots & \\text{ST\\_Slope\\_Up} \\\\\n\\hline\n40 & 140 & 289 & 0 & 172 & 0.0 & 0 & \\text{False} & \\text{True} & \\text{False} & \\ldots & \\text{True} \\\\\n49 & 160 & 180 & 0 & 156 & 1.0 & 1 & \\text{True} & \\text{False} & \\text{False} & \\ldots & \\text{False} \\\\\n37 & 130 & 283 & 0 & 98 & 0.0 & 0 & \\text{False} & \\text{True} & \\text{False} & \\ldots & \\text{True} \\\\\n48 & 138 & 214 & 0 & 108 & 1.5 & 1 & \\text{True} & \\text{False} & \\text{True} & \\ldots & \\text{False} \\\\\n54 & 150 & 195 & 0 & 122 & 0.0 & 0 & \\text{False} & \\text{True} & \\text{False} & \\ldots & \\text{True} \\\\\n\\hline\n\\end{array}\n\n\nNotice how each categorical column is gone and is replaced now by the types. For example, the column \"Sex\" became two columns: \"Sex\\_F\" and \"Sex\\_M\". Each record will either have True (or 1) in the Male cell or in the Female cell. The other will be False (0). Note that with our encoding method, we cannot have two True for both genders.\n\n\n\\subsubsection{Data Normalization, Standardization, and Scaling}\nMachine learning models often exhibit improved performance and faster convergence when features are properly valued. The main techniques to do so are Normalization, Standardization, and Scaling. Specifically, \\textbf{normalization} is a technique that transforms all of the data into numbers between 0 and 1. \\textbf{Standardization} is a technique that transforms features to have a mean of zero and a standard deviation of one. \\textbf{Scaling}, adjusts the range of values without changing the underlying distribution shape. In this course, we will only discuss the first method, i.e. normalization. In later courses, we will elaborate more about these concepts.\n\nWe will rely on the scikit-learn library in this part. Therefore,  run the following command in your terminal or command prompt:\n\n\\begin{lstlisting}\npip install scikit-learn\n\\end{lstlisting}\n\n\n\nNow Let us go back to the notebook. For the sake of simplicity, let us assume we will only work on 3 columns: 'Age', 'RestingBP', 'Cholesterol'. Let's visualize these datacolumns first:\n\\begin{lstlisting}[language=Python]\nmini_data = data[['Age', 'RestingBP', 'Cholesterol']]\nmini_data.head()\n\\end{lstlisting}\n\nThe output is the following:\n\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Age} & \\text{RestingBP} & \\text{Cholesterol} \\\\\n\\hline\n40 & 140 & 289 \\\\\n49 & 160 & 180 \\\\\n37 & 130 & 283 \\\\\n48 & 138 & 214 \\\\\n54 & 150 & 195 \\\\\n\\hline\n\\end{array}\n\n\nAfterward, let us use the normalization technique to transform this data:\n\n\\begin{lstlisting}[language=Python]\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Normalize the selected columns\nmini_data = pd.DataFrame(scaler.fit_transform(mini_data), columns=mini_data.columns)\n\\end{lstlisting}\n\nWe imported the library sklearn, which we just installed a minute ago. Then, we define a new MinMaxScaler and use it to transform the dataset columns.\n\n\n\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Age} & \\text{RestingBP} & \\text{Cholesterol} \\\\\n\\hline\n0.244898 & 0.70 & 0.479270 \\\\\n0.428571 & 0.80 & 0.298507 \\\\\n0.183673 & 0.65 & 0.469320 \\\\\n0.408163 & 0.69 & 0.354892 \\\\\n0.530612 & 0.75 & 0.323383 \\\\\n\\hline\n\\end{array}\n\nNotice how the data changed. The data is now presented as a range from 0 to 1. We can apply this to the whole data without specifying the columns by just replacing the variable mini\\_data with data.\n\\begin{lstlisting}[language=Python]\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Normalize the selected columns\ndata = pd.DataFrame(scaler.fit_transform(mini_data), columns=data.columns)\n\\end{lstlisting}\n\n\n\\subsubsection{Feature Selection}\nImagine dealing with a huge dataset consisting of hundreds or thousands of features. Definitely, this would consume time to train and introduce noise. Feature selection involves identifying and choosing the most relevant features for model training, reducing dimensionality and noise. Techniques like correlation analysis, feature importance ranking, or model-based selection help in selecting features that contribute significantly to the model's predictive power. For instance, according to certain analyses, we might decide that the ChestPainType can be removed from our columns because it does not affect the learning quality. This section will be covered in later courses.\n\n\\subsubsection{Outlier Detection and Treatment}\nOutliers, those data points standing far apart from the rest, can mess up model training. Techniques like z-score analysis or statistical modeling help in spotting and dealing with these outliers. This makes sure that the machine learning models aren\u2019t too affected by these odd values, making the models stronger and more reliable. This section will also be covered in later courses.\n\n\n\\subsection{Summary}\nIn this lesson, we explored the fundamental phases of Data Cleaning and Data Preprocessing, crucial steps in refining raw data before using it for machine learning models or statistical analyses. Data Cleaning involves identifying and fixing errors like missing values and duplicate records. Handling missing values includes strategies like deletion or filling using mean or advanced methods. Detection methods like isnull() and duplicated() help spot these issues, followed by actions like dropping or preserving duplicates. Data Preprocessing focuses on transforming raw data into a format suitable for machine learning models. It involves encoding categorical variables, normalization, scaling, and feature selection. For instance, one-hot encoding converts non-numerical data into a format that machine learning algorithms can understand. These processes are essential for improving data quality and usability, leading to better predictive model accuracy and efficiency.\n\n\n\n%", "metadata": {"source": "./full-course/aai.txt"}}}